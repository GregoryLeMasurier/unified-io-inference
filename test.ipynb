{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "path = \"/media/NLP/simple_manipulation\"\n",
    "train_path = os.path.join(path, \"train\")\n",
    "val_path = os.path.join(path, \"val\")\n",
    "test_path = os.path.join(path, \"test\")\n",
    "\n",
    "train = os.listdir(train_path)\n",
    "train = train[0:100]\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $element_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_path = os.path.join(train_path, train[1])\n",
    "action_path = os.path.join(element_path, \"action.pkl\")\n",
    "assert os.path.exists(action_path)\n",
    "\n",
    "with open(action_path, \"rb\") as f:\n",
    "    action = pickle.load(f)\n",
    "\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_quantizer import PoseQuantizer\n",
    "\n",
    "pq = PoseQuantizer(-0.5, 0.75, 100)\n",
    "print(\"Original: \" + str(action['pose0_position'][0]))\n",
    "enc = pq.encode_array(action['pose0_position'][0])\n",
    "print(\"Encoded: \" + str(enc))\n",
    "dec = pq.decode_array(enc)\n",
    "print(\"Decoded: \" + str(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_path = os.path.join(train_path, train[1])\n",
    "trajectory_path = os.path.join(element_path, \"trajectory.pkl\")\n",
    "assert os.path.exists(trajectory_path)\n",
    "\n",
    "with open(trajectory_path, \"rb\") as f:\n",
    "    trajectory = pickle.load(f)\n",
    "\n",
    "\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train:\n",
    "    element_path = os.path.join(train_path, data)\n",
    "    trajectory_path = os.path.join(element_path, \"trajectory.pkl\")\n",
    "    assert os.path.exists(trajectory_path)\n",
    "\n",
    "    with open(trajectory_path, \"rb\") as f:\n",
    "        trajectory = pickle.load(f)\n",
    "    print(trajectory['action_bounds'])\n",
    "\n",
    "#trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1041, 10, 7663, 10, 32022, 32028, 32056, 11981, 10, 32059, 32059, 32059, 32000, 1041, 10, 7663, 10, 32022, 32049, 32057, 11981, 10, 32059, 32059, 32000, 32085, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "        \"t5-base\", model_max_length=256, extra_ids=1200) \n",
    "\n",
    "res = tokenizer(\"action: pose: <extra_id_1177><extra_id_1171><extra_id_1143> rotation: <extra_id_1140><extra_id_1140><extra_id_1140><extra_id_1199> action: pose: <extra_id_1177><extra_id_1150><extra_id_1142> rotation: <extra_id_1140><extra_id_1140><extra_id_1199><extra_id_1114>\", max_length=27, truncation=True, padding='max_length')\n",
    "\n",
    "#tokenizer.decode(res)\n",
    "res\n",
    "#res = np.array(res)\n",
    "#print(res)\n",
    "#masks = np.ones((res.shape[-1], res.shape[-1]))\n",
    "#masks = np.triu(masks, 1)\n",
    "#masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE SHAPE: (128, 256, 3)\n",
      "IMAGE TENSOR SHAPE: (384, 384, 3)\n",
      "[[[ 1.9577874  2.1309524  2.3437037]\n",
      "  [ 1.9663497  2.1397057  2.3524182]\n",
      "  [ 2.0091617  2.1834733  2.3959913]\n",
      "  ...\n",
      "  [ 2.040557   2.2155695  2.4279447]\n",
      "  [ 1.9834745  2.1572127  2.3698473]\n",
      "  [ 1.9064132  2.0784314  2.2914162]]\n",
      "\n",
      " [[ 1.9663497  2.1397057  2.3524182]\n",
      "  [ 1.9535064  2.1265757  2.3393464]\n",
      "  [ 1.923538   2.0959384  2.3088453]\n",
      "  ...\n",
      "  [ 1.5838969  1.7487161  1.9631662]\n",
      "  [ 1.9363816  2.1090686  2.3219173]\n",
      "  [ 2.0690982  2.2447479  2.4569933]]\n",
      "\n",
      " [[ 2.0063074  2.1805553  2.3930862]\n",
      "  [ 1.9849015  2.1586719  2.3712997]\n",
      "  [ 1.903559   2.0755134  2.288511 ]\n",
      "  ...\n",
      "  [ 1.1767083  1.3324386  1.5487388]\n",
      "  [ 1.9135486  2.085726   2.2986784]\n",
      "  [ 2.2060962  2.384804   2.5964267]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  ...\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]]\n",
      "\n",
      " [[-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  ...\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]]\n",
      "\n",
      " [[-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  ...\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]\n",
      "  [-2.117904  -2.0357144 -1.8044444]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:45:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 17 tasks loaded\n"
     ]
    }
   ],
   "source": [
    "import process_data\n",
    "\n",
    "path = \"/media/NLP/simple_manipulation/train/\"\n",
    "element_path = os.path.join(path, '000000')\n",
    "process_data.processDataPoint(element_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:08:37.903823: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 15:08:37.903860: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 15:08:37.903864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "    # attributes\n",
       "    config = UnifiedIOConfig(vocab_size=33152, image_vocab_size=16384, image_patch_size=16, dtype='float32', emb_dim=512, num_heads=6, num_encoder_layers=8, num_decoder_layers=8, head_dim=64, mlp_dim=1024, mlp_activations=('gelu', 'linear'), dropout_rate=0.0, logits_via_embedding=True, float32_attention_logits=False, encoder_max_image_length=576, encoder_max_text_length=256, decoder_max_image_length=256, decoder_max_text_length=256, visual_backbone_type=None, visual_backbone_feature=None, default_image_size=(384, 384), num_seg_emb=2)\n",
       "    vae_config = VAEConfig(embed_dim=256, n_embed=16384, double_z=False, z_channels=256, resolution=256, in_channels=3, out_ch=3, ch=128, ch_mult=(1, 1, 2, 2, 4), num_res_blocks=2, attn_resolutions=(16,), dropout=0, dtype='float32')\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state, checkpoints\n",
    "from flax.training.common_utils import shard\n",
    "from uio import utils\n",
    "from uio import network\n",
    "from uio.configs import CONFIGS, VAE_CONFIG\n",
    "from uio.model import UnifiedIOModel\n",
    "\n",
    "def init_train_state(\n",
    "    model, params, learning_rate\n",
    ") -> train_state.TrainState:\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn = model.module.apply,\n",
    "        tx=optimizer,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "conf = CONFIGS[\"small\"]\n",
    "module = network.Transformer(config=conf, vae_config=VAE_CONFIG)\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnifiedIOModel()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UnifiedIOModel(module, text_decoder_length=28, image_decoder_length=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = utils.load_checkpoint(\"/home/greg/NLP-Final/unified-io-inference/base.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35285/1899788700.py:1: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  sum(p.size for p in jax.tree_leaves(params))/10e6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.7520771"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.size for p in jax.tree_leaves(params))/10e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:10:00.092182: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:497] The NVIDIA driver's CUDA version is 11.7 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "state = init_train_state(model, params, learning_rate=5e-5)#decrease by 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_class",
   "language": "python",
   "name": "nlp_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "78a98554bd959fe647588642884065a24bb8267d1904c1c950aa6b68cc3632ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
